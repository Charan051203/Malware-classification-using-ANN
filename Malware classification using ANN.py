# -*- coding: utf-8 -*-
"""Malware ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Undx40MSEVpHoSWR2z9hKL7bOmxhmVlx
"""

!pip install lime

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.utils import to_categorical
from sklearn.preprocessing import label_binarize
from itertools import cycle
import lime
import lime.lime_tabular

# Load the data
file_path = '/content/train_vec.csv'
malware = pd.read_csv(file_path)

malware.shape

malware.head()

# Preprocess the data
# Drop 'Id', 'byteFsize', and 'class' columns
X = malware.drop(['Id', 'BytFSize', 'Class'], axis=1)
y = malware['Class']

# Encode the target labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encode the target labels
label_mapping =  {
    1: "Ramnit",
    2: "Lollipop",
    3: "Kelihos_ver3",
    4: "Vundo",
    5: "Simda",
    6: "Tracur",
    7: "Kelihos_ver1",
    8: "Obfuscator.ACY",
    9: "Gatak",
}
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

ClassCount = malware["Class"].value_counts().sort_index()
ClassCount

import matplotlib.pyplot as plt
plt.figure(figsize=(4, 4))

plt.pie(x=ClassCount, labels=label_mapping.values(), autopct="%1.0f%%")
plt.title("Malware Class-labels")

plt.show()

"""Without Dropout"""

# Define the model architecture
dropout_rate=0.2
model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(y_train.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

"""With Dropout"""

dropout_rate=0.2
model = Sequential()
model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(128, activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(64, activation='relu'))
model.add(Dropout(dropout_rate))
model.add(Dense(y_train.shape[1], activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

"""Other optimizers with test, train accuracy"""

model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=64, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

"""Graphs and ExplainableAI"""

X_train.shape[1]

y_train.shape[1]

def create_model(optimizer='adam', dropout_rate=0.2):
    model = Sequential()
    model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(len(np.unique(y)), activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Create the model with specified hyperparameters
model = create_model(optimizer='adam', dropout_rate=0.2)

# Train the model
history = model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=32, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"Test accuracy: {accuracy}")

# Evaluate the model
y_pred = model.predict(X_test_scaled)  # Use scaled data for prediction
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predicted probabilities to class labels

# Convert y_test to class labels if it's in one-hot encoded format
if y_test.ndim == 2:
    y_test_classes = np.argmax(y_test, axis=1)
else:
    y_test_classes = y_test

print(classification_report(y_test_classes, y_pred_classes))
print(confusion_matrix(y_test_classes, y_pred_classes))

# Confusion Matrix
# Convert y_test to class labels if it's in one-hot encoded format
y_test_classes = np.argmax(y_test, axis=1)

conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=label_mapping.values(), yticklabels=label_mapping.values())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plot the Loss vs. Accuracy graph
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['loss'], label='Training Loss')
plt.title('Accuracy vs. Loss')
plt.xlabel('Accuracy')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Compute ROC curve and ROC area for each class
y_score = model.predict(X_test_scaled)
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(y_score.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve
plt.figure()
for i in range(len(np.unique(y))):
    plt.plot(fpr[i], tpr[i], label=f'ROC curve of class {i} (area = {roc_auc[i]:0.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Use LIME for interpretability
explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, training_labels=y_train, feature_names=X.columns, class_names=list(label_mapping.values()), mode='classification')
i = np.random.randint(0, X_test.shape[0])
exp = explainer.explain_instance(X_test.iloc[i].values, model.predict, num_features=10, top_labels=1)
exp.show_in_notebook(show_table=True, show_all=False)